[{"timestamp":1769987183282,"message":"Downloading llama-b7898-bin-win-cuda-13.1-x64.zip...","type":"info"},{"timestamp":1769987196535,"message":"File downloaded successfully: 146683556 bytes","type":"info"},{"timestamp":1769987196536,"message":"Extracting llama-b7898-bin-win-cuda-13.1-x64.zip...","type":"info"},{"timestamp":1769987196536,"message":"Extracting archive: C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\llama-b7898-bin-win-cuda-13.1-x64.zip (139.89 MB)","type":"info"},{"timestamp":1769987198167,"message":"Successfully extracted ZIP file","type":"success"},{"timestamp":1769987198167,"message":"Archive extraction complete: 39 items extracted","type":"success"},{"timestamp":1769987198179,"message":"Cleaned up archive file","type":"info"},{"timestamp":1769987198180,"message":"Installation of llama-b7898-bin-win-cuda-13.1-x64.zip completed","type":"success"},{"timestamp":1769987386132,"message":"Starting server with model: Qwen3-8B-Q4_K_M.gguf","type":"info"},{"timestamp":1769987386132,"message":"Command: C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\llama-server.exe -m C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf --port 8080 --host 127.0.0.1 -c 4096 -ngl 0 -t 24 -b 512","type":"info"},{"timestamp":1769987386209,"message":"load_backend: loaded RPC backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-rpc.dll","type":"error"},{"timestamp":1769987386240,"message":"load_backend: loaded CPU backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-cpu-zen4.dll","type":"error"},{"timestamp":1769987386241,"message":"main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true\r\nbuild: 7898 (89f10baad) with Clang 19.1.5 for Windows x86_64\r\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 24\r\n\r\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \r\n\r\nRunning without SSL\r\ninit: using 23 threads for HTTP server","type":"error"},{"timestamp":1769987386241,"message":"start: binding port with default address family","type":"error"},{"timestamp":1769987386246,"message":"main: loading model\r\nsrv    load_model: loading model 'C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf'\r\ncommon_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on","type":"error"},{"timestamp":1769987386430,"message":"llama_params_fit_impl: no devices with dedicated memory found\r\nllama_params_fit: successfully fit params to free device memory\r\nllama_params_fit: fitting params to free memory took 0.18 seconds","type":"error"},{"timestamp":1769987386453,"message":"llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\r\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\r\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\r\nllama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\r\nllama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\r\nllama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\r\nllama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\r\nllama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\r\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2","type":"error"},{"timestamp":1769987386470,"message":"llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...","type":"error"},{"timestamp":1769987386477,"message":"llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...","type":"error"},{"timestamp":1769987386496,"message":"llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:  145 tensors\r\nllama_model_loader: - type q4_K:  217 tensors\r\nllama_model_loader: - type q6_K:   37 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 4.68 GiB (4.90 BPW)","type":"error"},{"timestamp":1769987386550,"message":"load: 0 unused tokens","type":"error"},{"timestamp":1769987386559,"message":"load: printing all EOG tokens:\r\nload:   - 151643 ('<|endoftext|>')\r\nload:   - 151645 ('<|im_end|>')\r\nload:   - 151662 ('<|fim_pad|>')\r\nload:   - 151663 ('<|repo_name|>')\r\nload:   - 151664 ('<|file_sep|>')","type":"error"},{"timestamp":1769987386559,"message":"load: special tokens cache size = 26","type":"error"},{"timestamp":1769987386577,"message":"load: token to piece cache size = 0.9311 MB\r\nprint_info: arch                  = qwen3\r\nprint_info: vocab_only            = 0\r\nprint_info: no_alloc              = 0\r\nprint_info: n_ctx_train           = 32768\r\nprint_info: n_embd                = 4096\r\nprint_info: n_embd_inp            = 4096\r\nprint_info: n_layer               = 36\r\nprint_info: n_head                = 32\r\nprint_info: n_head_kv             = 8\r\nprint_info: n_rot                 = 128\r\nprint_info: n_swa                 = 0\r\nprint_info: is_swa_any            = 0\r\nprint_info: n_embd_head_k         = 128\r\nprint_info: n_embd_head_v         = 128\r\nprint_info: n_gqa                 = 4\r\nprint_info: n_embd_k_gqa          = 1024\r\nprint_info: n_embd_v_gqa          = 1024\r\nprint_info: f_norm_eps            = 0.0e+00\r\nprint_info: f_norm_rms_eps        = 1.0e-06\r\nprint_info: f_clamp_kqv           = 0.0e+00\r\nprint_info: f_max_alibi_bias      = 0.0e+00\r\nprint_info: f_logit_scale         = 0.0e+00\r\nprint_info: f_attn_scale          = 0.0e+00\r\nprint_info: n_ff                  = 12288\r\nprint_info: n_expert              = 0\r\nprint_info: n_expert_used         = 0\r\nprint_info: n_expert_groups       = 0\r\nprint_info: n_group_used          = 0\r\nprint_info: causal attn           = 1\r\nprint_info: pooling type          = -1\r\nprint_info: rope type             = 2\r\nprint_info: rope scaling          = linear\r\nprint_info: freq_base_train       = 1000000.0\r\nprint_info: freq_scale_train      = 1\r\nprint_info: n_ctx_orig_yarn       = 32768\r\nprint_info: rope_yarn_log_mul     = 0.0000\r\nprint_info: rope_finetuned        = unknown\r\nprint_info: model type            = 8B\r\nprint_info: model params          = 8.19 B\r\nprint_info: general.name          = Qwen3 8B\r\nprint_info: vocab type            = BPE\r\nprint_info: n_vocab               = 151936\r\nprint_info: n_merges              = 151387\r\nprint_info: BOS token             = 151643 '<|endoftext|>'\r\nprint_info: EOS token             = 151645 '<|im_end|>'\r\nprint_info: EOT token             = 151645 '<|im_end|>'","type":"error"},{"timestamp":1769987386577,"message":"print_info: PAD token             = 151643 '<|endoftext|>'\r\nprint_info: LF token              = 198 'Ċ'\r\nprint_info: FIM PRE token         = 151659 '<|fim_prefix|>'\r\nprint_info: FIM SUF token         = 151661 '<|fim_suffix|>'\r\nprint_info: FIM MID token         = 151660 '<|fim_middle|>'\r\nprint_info: FIM PAD token         = 151662 '<|fim_pad|>'\r\nprint_info: FIM REP token         = 151663 '<|repo_name|>'\r\nprint_info: FIM SEP token         = 151664 '<|file_sep|>'\r\nprint_info: EOG token             = 151643 '<|endoftext|>'\r\nprint_info: EOG token             = 151645 '<|im_end|>'\r\nprint_info: EOG token             = 151662 '<|fim_pad|>'\r\nprint_info: EOG token             = 151663 '<|repo_name|>'\r\nprint_info: EOG token             = 151664 '<|file_sep|>'\r\nprint_info: max token length      = 256\r\nload_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)","type":"error"},{"timestamp":1769987388065,"message":"load_tensors: offloading 0 repeating layers to GPU\r\nload_tensors: offloaded 0/37 layers to GPU\r\nload_tensors:   CPU_Mapped model buffer size =  4762.19 MiB\r\nload_tensors:   CPU_REPACK model buffer size =  3199.50 MiB\r\n...","type":"error"},{"timestamp":1769987388065,"message":"......","type":"error"},{"timestamp":1769987388065,"message":"...","type":"error"},{"timestamp":1769987388066,"message":"...","type":"error"},{"timestamp":1769987388066,"message":"...","type":"error"},{"timestamp":1769987388090,"message":".","type":"error"},{"timestamp":1769987388104,"message":"Stopping server...","type":"info"},{"timestamp":1769987388112,"message":"Starting server with model: Qwen3-8B-Q4_K_M.gguf","type":"info"},{"timestamp":1769987388113,"message":"Command: C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\llama-server.exe -m C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf --port 8080 --host 127.0.0.1 -c 4096 -ngl 0 -t 24 -b 512","type":"info"},{"timestamp":1769987388117,"message":".","type":"error"},{"timestamp":1769987388130,"message":"load_backend: loaded RPC backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-rpc.dll","type":"error"},{"timestamp":1769987388154,"message":".","type":"error"},{"timestamp":1769987388163,"message":"load_backend: loaded CPU backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-cpu-zen4.dll","type":"error"},{"timestamp":1769987388165,"message":"main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true\r\nbuild: 7898 (89f10baad) with Clang 19.1.5 for Windows x86_64\r\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 24\r\n\r\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \r\n\r\nRunning without SSL\r\ninit: using 23 threads for HTTP server","type":"error"},{"timestamp":1769987388165,"message":"start: binding port with default address family","type":"error"},{"timestamp":1769987388178,"message":"main: loading model\r\nsrv    load_model: loading model 'C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf'\r\ncommon_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on","type":"error"},{"timestamp":1769987388181,"message":".","type":"error"},{"timestamp":1769987388202,"message":".","type":"error"},{"timestamp":1769987388260,"message":"Server process exited with code 1","type":"error"},{"timestamp":1769987388366,"message":"llama_params_fit_impl: no devices with dedicated memory found\r\nllama_params_fit: successfully fit params to free device memory\r\nllama_params_fit: fitting params to free memory took 0.17 seconds","type":"error"},{"timestamp":1769987388388,"message":"llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\r\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\r\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\r\nllama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\r\nllama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\r\nllama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\r\nllama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\r\nllama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\r\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2","type":"error"},{"timestamp":1769987388406,"message":"llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...","type":"error"},{"timestamp":1769987388413,"message":"llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...","type":"error"},{"timestamp":1769987388432,"message":"llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:  145 tensors\r\nllama_model_loader: - type q4_K:  217 tensors\r\nllama_model_loader: - type q6_K:   37 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 4.68 GiB (4.90 BPW)","type":"error"},{"timestamp":1769987388490,"message":"load: 0 unused tokens","type":"error"},{"timestamp":1769987388504,"message":"load: printing all EOG tokens:\r\nload:   - 151643 ('<|endoftext|>')\r\nload:   - 151645 ('<|im_end|>')\r\nload:   - 151662 ('<|fim_pad|>')\r\nload:   - 151663 ('<|repo_name|>')\r\nload:   - 151664 ('<|file_sep|>')","type":"error"},{"timestamp":1769987388505,"message":"load: special tokens cache size = 26","type":"error"},{"timestamp":1769987388523,"message":"load: token to piece cache size = 0.9311 MB\r\nprint_info: arch                  = qwen3\r\nprint_info: vocab_only            = 0\r\nprint_info: no_alloc              = 0\r\nprint_info: n_ctx_train           = 32768\r\nprint_info: n_embd                = 4096\r\nprint_info: n_embd_inp            = 4096\r\nprint_info: n_layer               = 36\r\nprint_info: n_head                = 32\r\nprint_info: n_head_kv             = 8\r\nprint_info: n_rot                 = 128\r\nprint_info: n_swa                 = 0\r\nprint_info: is_swa_any            = 0\r\nprint_info: n_embd_head_k         = 128\r\nprint_info: n_embd_head_v         = 128\r\nprint_info: n_gqa                 = 4\r\nprint_info: n_embd_k_gqa          = 1024\r\nprint_info: n_embd_v_gqa          = 1024\r\nprint_info: f_norm_eps            = 0.0e+00\r\nprint_info: f_norm_rms_eps        = 1.0e-06\r\nprint_info: f_clamp_kqv           = 0.0e+00\r\nprint_info: f_max_alibi_bias      = 0.0e+00\r\nprint_info: f_logit_scale         = 0.0e+00\r\nprint_info: f_attn_scale          = 0.0e+00\r\nprint_info: n_ff                  = 12288\r\nprint_info: n_expert              = 0\r\nprint_info: n_expert_used         = 0\r\nprint_info: n_expert_groups       = 0\r\nprint_info: n_group_used          = 0\r\nprint_info: causal attn           = 1\r\nprint_info: pooling type          = -1\r\nprint_info: rope type             = 2\r\nprint_info: rope scaling          = linear\r\nprint_info: freq_base_train       = 1000000.0\r\nprint_info: freq_scale_train      = 1\r\nprint_info: n_ctx_orig_yarn       = 32768\r\nprint_info: rope_yarn_log_mul     = 0.0000\r\nprint_info: rope_finetuned        = unknown\r\nprint_info: model type            = 8B\r\nprint_info: model params          = 8.19 B\r\nprint_info: general.name          = Qwen3 8B\r\nprint_info: vocab type            = BPE\r\nprint_info: n_vocab               = 151936\r\nprint_info: n_merges              = 151387\r\nprint_info: BOS token             = 151643 '<|endoftext|>'\r\nprint_info: EOS token             = 151645 '<|im_end|>'\r\nprint_info: EOT token             = 151645 '<|im_end|>'\r\nprint_info: PAD token             = 151643 '<|endoftext|>'\r\nprint_info: LF token              = 198 'Ċ'\r\nprint_info: FIM PRE token         = 151659 '<|fim_prefix|>'\r\nprint_info: FIM SUF token         = 151661 '<|fim_suffix|>'\r\nprint_info: FIM MID token         = 151660 '<|fim_middle|>'\r\nprint_info: FIM PAD token         = 151662 '<|fim_pad|>'\r\nprint_info: FIM REP token         = 151663 '<|repo_name|>'\r\nprint_info: FIM SEP token         = 151664 '<|file_sep|>'\r\nprint_info: EOG token             = 151643 '<|endoftext|>'\r\nprint_info: EOG token             = 151645 '<|im_end|>'\r\nprint_info: EOG token             = 151662 '<|fim_pad|>'\r\nprint_info: EOG token             = 151663 '<|repo_name|>'\r\nprint_info: EOG token             = 151664 '<|file_sep|>'\r\nprint_info: max token length      = 256\r\nload_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)","type":"error"},{"timestamp":1769987388760,"message":"load_tensors: offloading 0 repeating layers to GPU\r\nload_tensors: offloaded 0/37 layers to GPU\r\nload_tensors:   CPU_Mapped model buffer size =  4762.19 MiB\r\nload_tensors:   CPU_REPACK model buffer size =  3199.50 MiB\r\n...","type":"error"},{"timestamp":1769987388761,"message":".....","type":"error"},{"timestamp":1769987388761,"message":"...","type":"error"},{"timestamp":1769987388761,"message":".....","type":"error"},{"timestamp":1769987388761,"message":"..","type":"error"},{"timestamp":1769987388784,"message":".","type":"error"},{"timestamp":1769987388810,"message":".","type":"error"},{"timestamp":1769987388840,"message":".","type":"error"},{"timestamp":1769987388866,"message":".","type":"error"},{"timestamp":1769987388885,"message":".","type":"error"},{"timestamp":1769987388921,"message":".","type":"error"},{"timestamp":1769987388947,"message":".","type":"error"},{"timestamp":1769987388977,"message":".","type":"error"},{"timestamp":1769987388989,"message":".","type":"error"},{"timestamp":1769987389019,"message":".","type":"error"},{"timestamp":1769987389045,"message":".","type":"error"},{"timestamp":1769987389074,"message":".","type":"error"},{"timestamp":1769987389101,"message":".","type":"error"},{"timestamp":1769987389130,"message":".","type":"error"},{"timestamp":1769987389158,"message":".","type":"error"},{"timestamp":1769987389172,"message":".","type":"error"},{"timestamp":1769987389198,"message":".","type":"error"},{"timestamp":1769987389228,"message":".","type":"error"},{"timestamp":1769987389255,"message":".","type":"error"},{"timestamp":1769987389284,"message":".","type":"error"},{"timestamp":1769987389312,"message":".","type":"error"},{"timestamp":1769987389341,"message":".","type":"error"},{"timestamp":1769987389367,"message":".","type":"error"},{"timestamp":1769987389382,"message":".","type":"error"},{"timestamp":1769987389409,"message":".","type":"error"},{"timestamp":1769987389438,"message":".","type":"error"},{"timestamp":1769987389466,"message":".","type":"error"},{"timestamp":1769987389495,"message":".","type":"error"},{"timestamp":1769987389521,"message":".","type":"error"},{"timestamp":1769987389541,"message":".","type":"error"},{"timestamp":1769987389577,"message":".","type":"error"},{"timestamp":1769987389592,"message":".","type":"error"},{"timestamp":1769987389620,"message":".","type":"error"},{"timestamp":1769987389649,"message":".","type":"error"},{"timestamp":1769987389675,"message":".","type":"error"},{"timestamp":1769987389698,"message":".","type":"error"},{"timestamp":1769987389732,"message":".","type":"error"},{"timestamp":1769987389752,"message":".","type":"error"},{"timestamp":1769987389787,"message":".","type":"error"},{"timestamp":1769987389802,"message":".","type":"error"},{"timestamp":1769987389827,"message":".","type":"error"},{"timestamp":1769987389854,"message":".","type":"error"},{"timestamp":1769987389884,"message":".","type":"error"},{"timestamp":1769987389906,"message":".","type":"error"},{"timestamp":1769987389942,"message":".","type":"error"},{"timestamp":1769987389962,"message":".","type":"error"},{"timestamp":1769987389998,"message":".","type":"error"},{"timestamp":1769987390025,"message":".","type":"error"},{"timestamp":1769987390040,"message":".","type":"error"},{"timestamp":1769987390068,"message":".","type":"error"},{"timestamp":1769987390097,"message":".","type":"error"},{"timestamp":1769987390116,"message":".","type":"error"},{"timestamp":1769987390151,"message":".","type":"error"},{"timestamp":1769987390177,"message":".","type":"error"},{"timestamp":1769987390205,"message":".","type":"error"},{"timestamp":1769987390218,"message":".","type":"error"},{"timestamp":1769987390246,"message":".","type":"error"},{"timestamp":1769987390271,"message":".","type":"error"},{"timestamp":1769987390300,"message":".","type":"error"},{"timestamp":1769987390324,"message":".","type":"error"},{"timestamp":1769987390345,"message":".","type":"error"},{"timestamp":1769987390378,"message":".","type":"error"},{"timestamp":1769987390404,"message":".","type":"error"},{"timestamp":1769987390423,"message":".","type":"error"},{"timestamp":1769987390457,"message":".","type":"error"},{"timestamp":1769987390482,"message":".","type":"error"},{"timestamp":1769987390497,"message":".","type":"error"},{"timestamp":1769987390500,"message":"common_init_result: added <|endoftext|> logit bias = -inf\r\ncommon_init_result: added <|im_end|> logit bias = -inf\r\ncommon_init_result: added <|fim_pad|> logit bias = -inf\r\ncommon_init_result: added <|repo_name|> logit bias = -inf\r\ncommon_init_result: added <|file_sep|> logit bias = -inf","type":"error"},{"timestamp":1769987390500,"message":"llama_context: constructing llama_context\r\nllama_context: n_seq_max     = 4\r\nllama_context: n_ctx         = 4096\r\nllama_context: n_ctx_seq     = 4096\r\nllama_context: n_batch       = 512\r\nllama_context: n_ubatch      = 512\r\nllama_context: causal_attn   = 1\r\nllama_context: flash_attn    = auto\r\nllama_context: kv_unified    = true\r\nllama_context: freq_base     = 1000000.0\r\nllama_context: freq_scale    = 1\r\nllama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\r\nllama_context:        CPU  output buffer size =     2.32 MiB","type":"error"},{"timestamp":1769987390500,"message":"llama_kv_cache:        CPU KV buffer size =   576.00 MiB","type":"error"},{"timestamp":1769987390580,"message":"llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  4/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\r\nsched_reserve: reserving ...","type":"error"},{"timestamp":1769987390582,"message":"sched_reserve: Flash Attention was auto, set to enabled","type":"error"},{"timestamp":1769987390583,"message":"sched_reserve:        CPU compute buffer size =   312.75 MiB\r\nsched_reserve: graph nodes  = 1267\r\nsched_reserve: graph splits = 1\r\nsched_reserve: reserve took 2.78 ms, sched copies = 1","type":"error"},{"timestamp":1769987390583,"message":"common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)","type":"error"},{"timestamp":1769987390904,"message":"srv    load_model: initializing slots, n_slots = 4\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  0 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  0 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  1 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  1 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  2 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  2 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  3 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  3 | task -1 | new slot, n_ctx = 4096\r\nsrv    load_model: prompt cache is enabled, size limit: 8192 MiB\r\nsrv    load_model: use `--cache-ram 0` to disable the prompt cache\r\nsrv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391","type":"error"},{"timestamp":1769987390907,"message":"init: chat template, example_format: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'","type":"error"},{"timestamp":1769987390908,"message":"srv          init: init: chat template, thinking = 1\r\nmain: model loaded\r\nmain: server is listening on http://127.0.0.1:8080\r\nmain: starting the main loop...\r\nsrv  update_slots: all slots are idle","type":"error"},{"timestamp":1769987407881,"message":"Starting server with model: Qwen3-8B-Q4_K_M.gguf","type":"info"},{"timestamp":1769987407881,"message":"Command: C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\llama-server.exe -m C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf --port 8080 --host 127.0.0.1 -c 4096 -ngl 0 -t 24 -b 512","type":"info"},{"timestamp":1769987407901,"message":"load_backend: loaded RPC backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-rpc.dll","type":"error"},{"timestamp":1769987407930,"message":"load_backend: loaded CPU backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-cpu-zen4.dll","type":"error"},{"timestamp":1769987407931,"message":"main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true\r\nbuild: 7898 (89f10baad) with Clang 19.1.5 for Windows x86_64\r\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 24\r\n\r\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \r\n\r\nRunning without SSL\r\ninit: using 23 threads for HTTP server\r\nstart: binding port with default address family","type":"error"},{"timestamp":1769987407948,"message":"main: loading model\r\nsrv    load_model: loading model 'C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf'\r\ncommon_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on","type":"error"},{"timestamp":1769987408145,"message":"llama_params_fit_impl: no devices with dedicated memory found\r\nllama_params_fit: successfully fit params to free device memory\r\nllama_params_fit: fitting params to free memory took 0.18 seconds","type":"error"},{"timestamp":1769987408171,"message":"llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\r\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\r\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\r\nllama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\r\nllama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\r\nllama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\r\nllama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\r\nllama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\r\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2","type":"error"},{"timestamp":1769987408190,"message":"llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...","type":"error"},{"timestamp":1769987408197,"message":"llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...","type":"error"},{"timestamp":1769987408216,"message":"llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:  145 tensors\r\nllama_model_loader: - type q4_K:  217 tensors\r\nllama_model_loader: - type q6_K:   37 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 4.68 GiB (4.90 BPW)","type":"error"},{"timestamp":1769987408282,"message":"load: 0 unused tokens","type":"error"},{"timestamp":1769987408293,"message":"load: printing all EOG tokens:\r\nload:   - 151643 ('<|endoftext|>')\r\nload:   - 151645 ('<|im_end|>')\r\nload:   - 151662 ('<|fim_pad|>')\r\nload:   - 151663 ('<|repo_name|>')\r\nload:   - 151664 ('<|file_sep|>')\r\nload: special tokens cache size = 26","type":"error"},{"timestamp":1769987408312,"message":"load: token to piece cache size = 0.9311 MB\r\nprint_info: arch                  = qwen3\r\nprint_info: vocab_only            = 0\r\nprint_info: no_alloc              = 0\r\nprint_info: n_ctx_train           = 32768\r\nprint_info: n_embd                = 4096\r\nprint_info: n_embd_inp            = 4096\r\nprint_info: n_layer               = 36\r\nprint_info: n_head                = 32\r\nprint_info: n_head_kv             = 8\r\nprint_info: n_rot                 = 128\r\nprint_info: n_swa                 = 0\r\nprint_info: is_swa_any            = 0\r\nprint_info: n_embd_head_k         = 128\r\nprint_info: n_embd_head_v         = 128\r\nprint_info: n_gqa                 = 4\r\nprint_info: n_embd_k_gqa          = 1024\r\nprint_info: n_embd_v_gqa          = 1024\r\nprint_info: f_norm_eps            = 0.0e+00\r\nprint_info: f_norm_rms_eps        = 1.0e-06\r\nprint_info: f_clamp_kqv           = 0.0e+00\r\nprint_info: f_max_alibi_bias      = 0.0e+00\r\nprint_info: f_logit_scale         = 0.0e+00\r\nprint_info: f_attn_scale          = 0.0e+00\r\nprint_info: n_ff                  = 12288\r\nprint_info: n_expert              = 0\r\nprint_info: n_expert_used         = 0\r\nprint_info: n_expert_groups       = 0\r\nprint_info: n_group_used          = 0\r\nprint_info: causal attn           = 1\r\nprint_info: pooling type          = -1\r\nprint_info: rope type             = 2\r\nprint_info: rope scaling          = linear\r\nprint_info: freq_base_train       = 1000000.0\r\nprint_info: freq_scale_train      = 1\r\nprint_info: n_ctx_orig_yarn       = 32768\r\nprint_info: rope_yarn_log_mul     = 0.0000\r\nprint_info: rope_finetuned        = unknown\r\nprint_info: model type            = 8B\r\nprint_info: model params          = 8.19 B\r\nprint_info: general.name          = Qwen3 8B\r\nprint_info: vocab type            = BPE\r\nprint_info: n_vocab               = 151936\r\nprint_info: n_merges              = 151387\r\nprint_info: BOS token             = 151643 '<|endoftext|>'\r\nprint_info: EOS token             = 151645 '<|im_end|>'\r\nprint_info: EOT token             = 151645 '<|im_end|>'\r\nprint_info: PAD token             = 151643 '<|endoftext|>'\r\nprint_info: LF token              = 198 'Ċ'\r\nprint_info: FIM PRE token         = 151659 '<|fim_prefix|>'\r\nprint_info: FIM SUF token         = 151661 '<|fim_suffix|>'\r\nprint_info: FIM MID token         = 151660 '<|fim_middle|>'\r\nprint_info: FIM PAD token         = 151662 '<|fim_pad|>'\r\nprint_info: FIM REP token         = 151663 '<|repo_name|>'","type":"error"},{"timestamp":1769987408312,"message":"print_info: FIM SEP token         = 151664 '<|file_sep|>'\r\nprint_info: EOG token             = 151643 '<|endoftext|>'\r\nprint_info: EOG token             = 151645 '<|im_end|>'\r\nprint_info: EOG token             = 151662 '<|fim_pad|>'\r\nprint_info: EOG token             = 151663 '<|repo_name|>'\r\nprint_info: EOG token             = 151664 '<|file_sep|>'\r\nprint_info: max token length      = 256\r\nload_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)","type":"error"},{"timestamp":1769987408552,"message":"load_tensors: offloading 0 repeating layers to GPU\r\nload_tensors: offloaded 0/37 layers to GPU\r\nload_tensors:   CPU_Mapped model buffer size =  4762.19 MiB\r\nload_tensors:   CPU_REPACK model buffer size =  3199.50 MiB\r\n............","type":"error"},{"timestamp":1769987408552,"message":"......","type":"error"},{"timestamp":1769987408577,"message":".","type":"error"},{"timestamp":1769987408601,"message":".","type":"error"},{"timestamp":1769987408628,"message":".","type":"error"},{"timestamp":1769987408652,"message":".","type":"error"},{"timestamp":1769987408670,"message":".","type":"error"},{"timestamp":1769987408702,"message":".","type":"error"},{"timestamp":1769987408727,"message":".","type":"error"},{"timestamp":1769987408753,"message":".","type":"error"},{"timestamp":1769987408764,"message":".","type":"error"},{"timestamp":1769987408791,"message":".","type":"error"},{"timestamp":1769987408814,"message":".","type":"error"},{"timestamp":1769987408841,"message":".","type":"error"},{"timestamp":1769987408867,"message":".","type":"error"},{"timestamp":1769987408895,"message":".","type":"error"},{"timestamp":1769987408920,"message":".","type":"error"},{"timestamp":1769987408934,"message":".","type":"error"},{"timestamp":1769987408958,"message":".","type":"error"},{"timestamp":1769987408986,"message":".","type":"error"},{"timestamp":1769987409013,"message":".","type":"error"},{"timestamp":1769987409041,"message":".","type":"error"},{"timestamp":1769987409068,"message":".","type":"error"},{"timestamp":1769987409097,"message":".","type":"error"},{"timestamp":1769987409122,"message":".","type":"error"},{"timestamp":1769987409136,"message":".","type":"error"},{"timestamp":1769987409163,"message":".","type":"error"},{"timestamp":1769987409191,"message":".","type":"error"},{"timestamp":1769987409217,"message":".","type":"error"},{"timestamp":1769987409245,"message":".","type":"error"},{"timestamp":1769987409270,"message":".","type":"error"},{"timestamp":1769987409289,"message":".","type":"error"},{"timestamp":1769987409324,"message":".","type":"error"},{"timestamp":1769987409338,"message":".","type":"error"},{"timestamp":1769987409364,"message":".","type":"error"},{"timestamp":1769987409391,"message":".","type":"error"},{"timestamp":1769987409416,"message":".","type":"error"},{"timestamp":1769987409436,"message":".","type":"error"},{"timestamp":1769987409470,"message":".","type":"error"},{"timestamp":1769987409489,"message":".","type":"error"},{"timestamp":1769987409525,"message":".","type":"error"},{"timestamp":1769987409541,"message":".","type":"error"},{"timestamp":1769987409566,"message":".","type":"error"},{"timestamp":1769987409592,"message":".","type":"error"},{"timestamp":1769987409619,"message":".","type":"error"},{"timestamp":1769987409639,"message":".","type":"error"},{"timestamp":1769987409670,"message":".","type":"error"},{"timestamp":1769987409688,"message":".","type":"error"},{"timestamp":1769987409721,"message":".","type":"error"},{"timestamp":1769987409746,"message":".","type":"error"},{"timestamp":1769987409759,"message":".","type":"error"},{"timestamp":1769987409784,"message":".","type":"error"},{"timestamp":1769987409813,"message":".","type":"error"},{"timestamp":1769987409831,"message":".","type":"error"},{"timestamp":1769987409863,"message":".","type":"error"},{"timestamp":1769987409888,"message":".","type":"error"},{"timestamp":1769987409915,"message":".","type":"error"},{"timestamp":1769987409926,"message":".","type":"error"},{"timestamp":1769987409954,"message":".","type":"error"},{"timestamp":1769987409978,"message":".","type":"error"},{"timestamp":1769987410005,"message":".","type":"error"},{"timestamp":1769987410029,"message":".","type":"error"},{"timestamp":1769987410048,"message":".","type":"error"},{"timestamp":1769987410080,"message":".","type":"error"},{"timestamp":1769987410103,"message":".","type":"error"},{"timestamp":1769987410121,"message":".","type":"error"},{"timestamp":1769987410154,"message":".","type":"error"},{"timestamp":1769987410179,"message":".","type":"error"},{"timestamp":1769987410193,"message":".","type":"error"},{"timestamp":1769987410195,"message":"common_init_result: added <|endoftext|> logit bias = -inf\r\ncommon_init_result: added <|im_end|> logit bias = -inf\r\ncommon_init_result: added <|fim_pad|> logit bias = -inf\r\ncommon_init_result: added <|repo_name|> logit bias = -inf\r\ncommon_init_result: added <|file_sep|> logit bias = -inf","type":"error"},{"timestamp":1769987410196,"message":"llama_context: constructing llama_context\r\nllama_context: n_seq_max     = 4\r\nllama_context: n_ctx         = 4096\r\nllama_context: n_ctx_seq     = 4096\r\nllama_context: n_batch       = 512\r\nllama_context: n_ubatch      = 512\r\nllama_context: causal_attn   = 1\r\nllama_context: flash_attn    = auto\r\nllama_context: kv_unified    = true\r\nllama_context: freq_base     = 1000000.0\r\nllama_context: freq_scale    = 1\r\nllama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\r\nllama_context:        CPU  output buffer size =     2.32 MiB","type":"error"},{"timestamp":1769987410196,"message":"llama_kv_cache:        CPU KV buffer size =   576.00 MiB","type":"error"},{"timestamp":1769987410277,"message":"llama_kv_cache: size =  576.00 MiB (  4096 cells,  36 layers,  4/1 seqs), K (f16):  288.00 MiB, V (f16):  288.00 MiB\r\nsched_reserve: reserving ...","type":"error"},{"timestamp":1769987410279,"message":"sched_reserve: Flash Attention was auto, set to enabled","type":"error"},{"timestamp":1769987410280,"message":"sched_reserve:        CPU compute buffer size =   312.75 MiB\r\nsched_reserve: graph nodes  = 1267\r\nsched_reserve: graph splits = 1\r\nsched_reserve: reserve took 2.68 ms, sched copies = 1","type":"error"},{"timestamp":1769987410280,"message":"common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)","type":"error"},{"timestamp":1769987410550,"message":"srv    load_model: initializing slots, n_slots = 4\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  0 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  0 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  1 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  1 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  2 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  2 | task -1 | new slot, n_ctx = 4096\r\nno implementations specified for speculative decoding\r\nslot   load_model: id  3 | task -1 | speculative decoding context not initialized\r\nslot   load_model: id  3 | task -1 | new slot, n_ctx = 4096\r\nsrv    load_model: prompt cache is enabled, size limit: 8192 MiB\r\nsrv    load_model: use `--cache-ram 0` to disable the prompt cache\r\nsrv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391","type":"error"},{"timestamp":1769987410553,"message":"init: chat template, example_format: '<|im_start|>system\r\nYou are a helpful assistant<|im_end|>\r\n<|im_start|>user\r\nHello<|im_end|>\r\n<|im_start|>assistant\r\nHi there<|im_end|>\r\n<|im_start|>user\r\nHow are you?<|im_end|>\r\n<|im_start|>assistant\r\n'","type":"error"},{"timestamp":1769987410554,"message":"srv          init: init: chat template, thinking = 1\r\nmain: model loaded\r\nmain: server is listening on http://127.0.0.1:8080\r\nmain: starting the main loop...\r\nsrv  update_slots: all slots are idle","type":"error"},{"timestamp":1769987414239,"message":"Stopping server...","type":"info"},{"timestamp":1769987414247,"message":"Starting server with model: Qwen3-8B-Q4_K_M.gguf","type":"info"},{"timestamp":1769987414247,"message":"Command: C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\llama-server.exe -m C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf --port 8080 --host 127.0.0.1 -c 4096 -ngl 0 -t 24 -b 512","type":"info"},{"timestamp":1769987414272,"message":"load_backend: loaded RPC backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-rpc.dll","type":"error"},{"timestamp":1769987414302,"message":"load_backend: loaded CPU backend from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\bin\\ggml-cpu-zen4.dll","type":"error"},{"timestamp":1769987414303,"message":"main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true\r\nbuild: 7898 (89f10baad) with Clang 19.1.5 for Windows x86_64\r\nsystem info: n_threads = 24, n_threads_batch = 24, total_threads = 24\r\n\r\nsystem_info: n_threads = 24 (n_threads_batch = 24) / 24 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \r\n\r\nRunning without SSL\r\ninit: using 23 threads for HTTP server","type":"error"},{"timestamp":1769987414304,"message":"start: binding port with default address family","type":"error"},{"timestamp":1769987414314,"message":"main: loading model\r\nsrv    load_model: loading model 'C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf'\r\ncommon_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on","type":"error"},{"timestamp":1769987414504,"message":"llama_params_fit_impl: no devices with dedicated memory found\r\nllama_params_fit: successfully fit params to free device memory\r\nllama_params_fit: fitting params to free memory took 0.18 seconds","type":"error"},{"timestamp":1769987414527,"message":"llama_model_loader: loaded meta data with 28 key-value pairs and 399 tensors from C:\\Users\\Smoffyy\\Desktop\\Llama.cpp\\models\\Qwen3-8B-Q4_K_M.gguf (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\r\nllama_model_loader: - kv   0:                       general.architecture str              = qwen3\r\nllama_model_loader: - kv   1:                               general.type str              = model\r\nllama_model_loader: - kv   2:                               general.name str              = Qwen3 8B\r\nllama_model_loader: - kv   3:                           general.basename str              = Qwen3\r\nllama_model_loader: - kv   4:                         general.size_label str              = 8B\r\nllama_model_loader: - kv   5:                            general.license str              = apache-2.0\r\nllama_model_loader: - kv   6:                          qwen3.block_count u32              = 36\r\nllama_model_loader: - kv   7:                       qwen3.context_length u32              = 32768\r\nllama_model_loader: - kv   8:                     qwen3.embedding_length u32              = 4096\r\nllama_model_loader: - kv   9:                  qwen3.feed_forward_length u32              = 12288\r\nllama_model_loader: - kv  10:                 qwen3.attention.head_count u32              = 32\r\nllama_model_loader: - kv  11:              qwen3.attention.head_count_kv u32              = 8\r\nllama_model_loader: - kv  12:                       qwen3.rope.freq_base f32              = 1000000.000000\r\nllama_model_loader: - kv  13:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\r\nllama_model_loader: - kv  14:                 qwen3.attention.key_length u32              = 128\r\nllama_model_loader: - kv  15:               qwen3.attention.value_length u32              = 128\r\nllama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\r\nllama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2","type":"error"},{"timestamp":1769987414545,"message":"llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...","type":"error"},{"timestamp":1769987414552,"message":"llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...","type":"error"},{"timestamp":1769987414573,"message":"llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\r\nllama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\r\nllama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\r\nllama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\r\nllama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = false\r\nllama_model_loader: - kv  25:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\r\nllama_model_loader: - kv  26:               general.quantization_version u32              = 2\r\nllama_model_loader: - kv  27:                          general.file_type u32              = 15\r\nllama_model_loader: - type  f32:  145 tensors\r\nllama_model_loader: - type q4_K:  217 tensors\r\nllama_model_loader: - type q6_K:   37 tensors\r\nprint_info: file format = GGUF V3 (latest)\r\nprint_info: file type   = Q4_K - Medium\r\nprint_info: file size   = 4.68 GiB (4.90 BPW)","type":"error"},{"timestamp":1769987414625,"message":"load: 0 unused tokens","type":"error"},{"timestamp":1769987414636,"message":"load: printing all EOG tokens:\r\nload:   - 151643 ('<|endoftext|>')\r\nload:   - 151645 ('<|im_end|>')\r\nload:   - 151662 ('<|fim_pad|>')\r\nload:   - 151663 ('<|repo_name|>')\r\nload:   - 151664 ('<|file_sep|>')","type":"error"},{"timestamp":1769987414636,"message":"load: special tokens cache size = 26","type":"error"},{"timestamp":1769987414655,"message":"load: token to piece cache size = 0.9311 MB\r\nprint_info: arch                  = qwen3\r\nprint_info: vocab_only            = 0\r\nprint_info: no_alloc              = 0\r\nprint_info: n_ctx_train           = 32768\r\nprint_info: n_embd                = 4096\r\nprint_info: n_embd_inp            = 4096\r\nprint_info: n_layer               = 36\r\nprint_info: n_head                = 32\r\nprint_info: n_head_kv             = 8\r\nprint_info: n_rot                 = 128\r\nprint_info: n_swa                 = 0\r\nprint_info: is_swa_any            = 0\r\nprint_info: n_embd_head_k         = 128\r\nprint_info: n_embd_head_v         = 128\r\nprint_info: n_gqa                 = 4\r\nprint_info: n_embd_k_gqa          = 1024\r\nprint_info: n_embd_v_gqa          = 1024\r\nprint_info: f_norm_eps            = 0.0e+00\r\nprint_info: f_norm_rms_eps        = 1.0e-06\r\nprint_info: f_clamp_kqv           = 0.0e+00\r\nprint_info: f_max_alibi_bias      = 0.0e+00\r\nprint_info: f_logit_scale         = 0.0e+00\r\nprint_info: f_attn_scale          = 0.0e+00\r\nprint_info: n_ff                  = 12288\r\nprint_info: n_expert              = 0\r\nprint_info: n_expert_used         = 0\r\nprint_info: n_expert_groups       = 0\r\nprint_info: n_group_used          = 0\r\nprint_info: causal attn           = 1\r\nprint_info: pooling type          = -1\r\nprint_info: rope type             = 2\r\nprint_info: rope scaling          = linear\r\nprint_info: freq_base_train       = 1000000.0\r\nprint_info: freq_scale_train      = 1\r\nprint_info: n_ctx_orig_yarn       = 32768\r\nprint_info: rope_yarn_log_mul     = 0.0000\r\nprint_info: rope_finetuned        = unknown\r\nprint_info: model type            = 8B\r\nprint_info: model params          = 8.19 B\r\nprint_info: general.name          = Qwen3 8B\r\nprint_info: vocab type            = BPE\r\nprint_info: n_vocab               = 151936\r\nprint_info: n_merges              = 151387\r\nprint_info: BOS token             = 151643 '<|endoftext|>'\r\nprint_info: EOS token             = 151645 '<|im_end|>'\r\nprint_info: EOT token             = 151645 '<|im_end|>'\r\nprint_info: PAD token             = 151643 '<|endoftext|>'\r\nprint_info: LF token              = 198 'Ċ'\r\nprint_info: FIM PRE token         = 151659 '<|fim_prefix|>'\r\nprint_info: FIM SUF token         = 151661 '<|fim_suffix|>'\r\nprint_info: FIM MID token         = 151660 '<|fim_middle|>'\r\nprint_info: FIM PAD token         = 151662 '<|fim_pad|>'\r\nprint_info: FIM REP token         = 151663 '<|repo_name|>'\r\nprint_info: FIM SEP token         = 151664 '<|file_sep|>'\r\nprint_info: EOG token             = 151643 '<|endoftext|>'\r\nprint_info: EOG token             = 151645 '<|im_end|>'\r\nprint_info: EOG token             = 151662 '<|fim_pad|>'\r\nprint_info: EOG token             = 151663 '<|repo_name|>'\r\nprint_info: EOG token             = 151664 '<|file_sep|>'\r\nprint_info: max token length      = 256\r\nload_tensors: loading model tensors, this can take a while... (mmap = true, direct_io = false)","type":"error"},{"timestamp":1769987414811,"message":"Server process exited with code 1","type":"error"},{"timestamp":1769987414959,"message":"load_tensors: offloading 0 repeating layers to GPU\r\nload_tensors: offloaded 0/37 layers to GPU\r\nload_tensors:   CPU_Mapped model buffer size =  4762.19 MiB\r\nload_tensors:   CPU_REPACK model buffer size =  3199.50 MiB\r\n...","type":"error"},{"timestamp":1769987414959,"message":".............","type":"error"},{"timestamp":1769987414959,"message":"..","type":"error"},{"timestamp":1769987414982,"message":".","type":"error"},{"timestamp":1769987415004,"message":".","type":"error"},{"timestamp":1769987415028,"message":".","type":"error"},{"timestamp":1769987415049,"message":".","type":"error"},{"timestamp":1769987415065,"message":".","type":"error"},{"timestamp":1769987415094,"message":".","type":"error"},{"timestamp":1769987415115,"message":".","type":"error"},{"timestamp":1769987415138,"message":".","type":"error"},{"timestamp":1769987415149,"message":".","type":"error"},{"timestamp":1769987415172,"message":".","type":"error"},{"timestamp":1769987415193,"message":".","type":"error"},{"timestamp":1769987415217,"message":".","type":"error"},{"timestamp":1769987415239,"message":".","type":"error"},{"timestamp":1769987415262,"message":".","type":"error"},{"timestamp":1769987415284,"message":".","type":"error"},{"timestamp":1769987415296,"message":".","type":"error"},{"timestamp":1769987415317,"message":".","type":"error"},{"timestamp":1769987415341,"message":".","type":"error"},{"timestamp":1769987415363,"message":".","type":"error"},{"timestamp":1769987415386,"message":".","type":"error"},{"timestamp":1769987415408,"message":".","type":"error"},{"timestamp":1769987415432,"message":".","type":"error"},{"timestamp":1769987415453,"message":".","type":"error"},{"timestamp":1769987415465,"message":".","type":"error"},{"timestamp":1769987415488,"message":".","type":"error"},{"timestamp":1769987415512,"message":".","type":"error"},{"timestamp":1769987415534,"message":".","type":"error"},{"timestamp":1769987415558,"message":".","type":"error"},{"timestamp":1769987415579,"message":".","type":"error"},{"timestamp":1769987415595,"message":".","type":"error"},{"timestamp":1769987415624,"message":".","type":"error"},{"timestamp":1769987415636,"message":".","type":"error"},{"timestamp":1769987415658,"message":".","type":"error"},{"timestamp":1769987415682,"message":".","type":"error"},{"timestamp":1769987415702,"message":".","type":"error"},{"timestamp":1769987415720,"message":".","type":"error"},{"timestamp":1769987415748,"message":".","type":"error"},{"timestamp":1769987415764,"message":".","type":"error"},{"timestamp":1769987415792,"message":".","type":"error"},{"timestamp":1769987415804,"message":".","type":"error"},{"timestamp":1769987415825,"message":".","type":"error"},{"timestamp":1769987415847,"message":".","type":"error"},{"timestamp":1769987415870,"message":".","type":"error"},{"timestamp":1769987415887,"message":".","type":"error"},{"timestamp":1769987415920,"message":".","type":"error"},{"timestamp":1769987415941,"message":".","type":"error"},{"timestamp":1769987415980,"message":".","type":"error"},{"timestamp":1769987416010,"message":".","type":"error"},{"timestamp":1769987416025,"message":".","type":"error"},{"timestamp":1769987416055,"message":".","type":"error"},{"timestamp":1769987416086,"message":".","type":"error"},{"timestamp":1769987416107,"message":".","type":"error"},{"timestamp":1769987416144,"message":".","type":"error"},{"timestamp":1769987416168,"message":".","type":"error"},{"timestamp":1769987416194,"message":".","type":"error"},{"timestamp":1769987416206,"message":".","type":"error"},{"timestamp":1769987416232,"message":".","type":"error"},{"timestamp":1769987416257,"message":".","type":"error"},{"timestamp":1769987416284,"message":".","type":"error"},{"timestamp":1769987416307,"message":".","type":"error"},{"timestamp":1769987416326,"message":".","type":"error"},{"timestamp":1769987416357,"message":".","type":"error"},{"timestamp":1769987416380,"message":".","type":"error"},{"timestamp":1769987416397,"message":".","type":"error"},{"timestamp":1769987416429,"message":".","type":"error"},{"timestamp":1769987416452,"message":".","type":"error"},{"timestamp":1769987416466,"message":".","type":"error"},{"timestamp":1769987416469,"message":"common_init_result: added <|endoftext|> logit bias = -inf\r\ncommon_init_result: added <|im_end|> logit bias = -inf\r\ncommon_init_result: added <|fim_pad|> logit bias = -inf\r\ncommon_init_result: added <|repo_name|> logit bias = -inf\r\ncommon_init_result: added <|file_sep|> logit bias = -inf","type":"error"},{"timestamp":1769987416470,"message":"llama_context: constructing llama_context\r\nllama_context: n_seq_max     = 4\r\nllama_context: n_ctx         = 4096\r\nllama_context: n_ctx_seq     = 4096\r\nllama_context: n_batch       = 512\r\nllama_context: n_ubatch      = 512\r\nllama_context: causal_attn   = 1\r\nllama_context: flash_attn    = auto\r\nllama_context: kv_unified    = true\r\nllama_context: freq_base     = 1000000.0\r\nllama_context: freq_scale    = 1\r\nllama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\r\nllama_context:        CPU  output buffer size =     2.32 MiB","type":"error"},{"timestamp":1769987416470,"message":"llama_kv_cache:        CPU KV buffer size =   576.00 MiB","type":"error"}]